{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort and Convert to FASTQ\n",
    "\n",
    "Sorts a BAM file and converts to FASTQ.\n",
    "The input file is first checked for paired reads count. If its non zero its treated as a paired-end file.\n",
    "Based on the user flag to split into R1 and R2 the paired file is separated into two fastq file. Else a single fastq file is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import pysam\n",
    "from hops import hdfs\n",
    "from pyspark import SparkContext\n",
    "\n",
    "import utils\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### load arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args=utils.load_arguments([0,'hdfs:///Projects/TCGA_viruses/Jupyter/pipeline/settings_DJ.yml'])\n",
    "args=utils.load_arguments(sys.argv)\n",
    "if args is not None:    \n",
    "    args=args[utils.KEY_SORTCONVERT]\n",
    "else :\n",
    "    sys.exit(utils.NO_CONFIG_ERR)\n",
    "    \n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "SPACE=utils.SPACE\n",
    "\n",
    "inputRoot=args['INPUT_ROOT']\n",
    "OUTPUT_FASTQ=args['OUTPUT_FASTQ']\n",
    "THREADS=str(args['THREADS'])\n",
    "# split separate R1 and R2 if paired file\n",
    "is_split_R1R2=args['SPLIT_PAIRS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count paired  in BAM reads countfile using pysam.\n",
    "Returns True if non zero.\n",
    "\"\"\"\n",
    "def count_paired_reads(x):\n",
    "     \n",
    "    o=pysam.view(x, '-c','-f','1',catch_stdout=True)\n",
    "    if int(o)!=0: # if non zero paired reads count\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Counts paired reads count in BAM using samtools.\n",
    "Returns True if non zero.\n",
    "\"\"\"\n",
    "def isPaired(file):\n",
    "\n",
    "    cmd1=utils.SAMTOOLS+' view -c -f 1 '+file \n",
    "    print(cmd1)\n",
    "    p1=subprocess.Popen(cmd1.split(),stdout=subprocess.PIPE)\n",
    "    cmd2='head -n 1'\n",
    "    p2=subprocess.run(cmd2.split(),stdin=p1.stdout,stdout=subprocess.PIPE)\n",
    "    if p1.stdout  :\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Sort given BAM file using pysam\n",
    "\"\"\"\n",
    "def sort(file):        \n",
    "    sort_file = utils.SORTED_PREFIX+file\n",
    "    pysam.sort('-@',THREADS,'-m','2G','-n',file,'-o',sort_file,catch_stdout=False)\n",
    "    return sort_file\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Map function to run on single BAM file.\n",
    "First the file is sorted and then the sorted file is converted to \n",
    "single or paired FASTQ files depending on user argument \n",
    "and if file is paired or not.\n",
    "Output is copied back to hdfs.\n",
    "If output file name is already present in destination the process is skipped \n",
    "to avoid processing of same file incase of resubmit of failed run.\n",
    "\n",
    "\n",
    "P.S/TODO: Commented code is to run via samtools and stays due to hadoop18 problem \n",
    "of running programs via subproces.\n",
    "Once hadoop18 is fixed samtools can be uncommented and pysam code then can be subsequently deleted\n",
    "\"\"\"\n",
    "def sort_convert_fastq(file_path):\n",
    "    file=os.path.split(file_path)[1]\n",
    "    filename=os.path.splitext(file)[0]      \n",
    "    output=filename+'.fq.gz'\n",
    "    if not hdfs.exists(os.path.join(OUTPUT_FASTQ,output)): # check if output already exists\n",
    "               \n",
    "    \n",
    "        hdfs.copy_to_local(file_path, overwrite=True)\n",
    "         \n",
    "        out_file_r1=''\n",
    "        out_file_r2=''\n",
    "        out_file=''\n",
    "\n",
    "\n",
    "         # check if bam file has paired reads\n",
    "        isPairedFile=count_paired_reads(file)\n",
    "        print('is paired ? ', isPairedFile)\n",
    "\n",
    "        # sort \n",
    "        print(\"INFO: Run sort \", file)    \n",
    "        sort_file=sort(file)\n",
    "        os.remove(file)\n",
    "        os.rename(sort_file,sort_file.split(utils.SORTED_PREFIX)[1])\n",
    "\n",
    "        # if the bam file is paired and user wants to split into separate R1 and R2 fastq\n",
    "        if (isPairedFile and is_split_R1R2):\n",
    "            out_file_r1=filename+'_R1.fq.gz'\n",
    "            out_file_r2=filename+'_R2.fq.gz'\n",
    "            #params={'fastq':file,'-1':out_file_r1,'-2':out_file_r2, '-0':'/dev/null', '-s':'/dev/null', '-c':1 ,'-@':THREADS}    \n",
    "            pysam.fastq(file,'-1',out_file_r1,'-2',out_file_r2, '-0','/dev/null', '-s','/dev/null', '-c','1','-@',THREADS,'-N',catch_stdout=False)\n",
    "\n",
    "\n",
    "        else:   # else if the file is single read or the user wants to split into a single fastq      \n",
    "            out_file=filename+'.fq.gz'\n",
    "            #params={'fastq': file,'-0': out_file, '-s':'/dev/null', '-c':1, '-@': THREADS}\n",
    "            if isPairedFile: # samtools fastq has two different argument values for output if the file is paired or not\n",
    "                OUT_ARG='-o'\n",
    "            else :\n",
    "                OUT_ARG='-0'\n",
    "            pysam.fastq(file, OUT_ARG, out_file,  '-c','1','-@',THREADS,'-N',catch_stdout=False)\n",
    "\n",
    "\n",
    "    # convert to fastq command  using samtools\n",
    "    #     cmdConvert=utils.build_command(utils.SAMTOOLS,params)  \n",
    "    #     print('INFO: Run sort and convert', cmdConvert)\n",
    "    #     p1 = subprocess.run(cmdConvert.split(), stdout=subprocess.PIPE) # run convert from pipe from sort command\n",
    "\n",
    "        os.remove(file)\n",
    "        if os.path.exists(out_file): # either a single fastq exists or separate R1 R2 fastq files\n",
    "            hdfs.copy_to_hdfs(out_file,OUTPUT_FASTQ,overwrite=True)\n",
    "            os.remove(out_file)\n",
    "            return True\n",
    "        if os.path.exists(out_file_r1):\n",
    "            hdfs.copy_to_hdfs(out_file_r1,OUTPUT_FASTQ,overwrite=True)\n",
    "            hdfs.copy_to_hdfs(out_file_r2,OUTPUT_FASTQ,overwrite=True)\n",
    "            os.remove(out_file_r1)\n",
    "            os.remove(out_file_r2)\n",
    "            return True\n",
    "\n",
    "\n",
    "        return False\n",
    "    \n",
    "    else:\n",
    "        print('skipping existing file: ', filename)            \n",
    "        return None\n",
    "\n",
    "        \n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load input files hdfs paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFiles=utils.load_file_names(inputRoot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "finalList = sc.parallelize(inputFiles).map(sort_convert_fastq).collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}