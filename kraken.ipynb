{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kraken2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import os\n",
    "from hops import hdfs\n",
    "import utils\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "import subprocess\n",
    "import stat\n",
    "import gzip\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args=utils.load_arguments([0,'hdfs:///Projects/TCGA_viruses/Jupyter/settings_DJ.yml'])\n",
    "args=utils.load_arguments(sys.argv)\n",
    "if args is not None:\n",
    "    args=args['Kraken']\n",
    "else :\n",
    "    sys.exit(utils.NO_CONFIG_ERR)\n",
    "    \n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "\n",
    "inputRoot=args['INPUT_ROOT']\n",
    "outputRoot=args['OUTPUT_ROOT']\n",
    "\n",
    "kraken_path=args['KRAKEN_PATH']\n",
    "tool=os.path.basename(kraken_path)\n",
    "kk_db_path=args['KRAKEN_DB_PATH']\n",
    "is_save_all_outputs=args['SAVE_FULL_OUTPUT']\n",
    "threads=args['THREADS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install kraken from hdfs source\n",
    "def load_kraken(kraken_path):\n",
    "    tool=os.path.basename(kraken_path)\n",
    "    \n",
    "    hdfs.copy_to_local(kraken_path)\n",
    "\n",
    "    st = os.stat(tool+'/kraken2')\n",
    "    os.chmod(tool+'/kraken2', st.st_mode | stat.S_IEXEC)\n",
    "    \n",
    "    st = os.stat(tool+'/classify')\n",
    "    os.chmod(tool+'/classify', st.st_mode | stat.S_IEXEC)\n",
    "\n",
    "\n",
    "\n",
    "def compress_file(file):\n",
    "    compress_file=file+'.gz'\n",
    "    with open(file, 'rb') as f_in:        \n",
    "        with gzip.open(compress_file, 'wb',compresslevel=1) as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "            \n",
    "    return compress_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Runds kraken on single file via subprcess.\n",
    "First kraken is installed by copying kraken tool from hdfs.\n",
    "Outputs are copied back to hdfs.\n",
    "If an output file name is already present in output directory the processing\n",
    "of file is skipped to avoid processing of same file in case of resubmit of failed run.\n",
    "\"\"\"\n",
    "def apply_kraken_single(file_path,kk_db_path): \n",
    "    \n",
    "    file=os.path.split(file_path)[1]  \n",
    "    sample=os.path.splitext(os.path.splitext(file)[0])[0]    \n",
    "    report=sample+'_report.txt'\n",
    "    \n",
    "    if not hdfs.exists(os.path.join(outputRoot,'report',report)): # check if output already exists\n",
    "    \n",
    "        load_kraken(kraken_path) # install kraken\n",
    "\n",
    "        kk_db=os.path.split(kk_db_path)[1]  \n",
    "        if not os.path.exists(kk_db):\n",
    "            hdfs.copy_to_local(kk_db_path)\n",
    "\n",
    "\n",
    "        hdfs.copy_to_local(file_path)\n",
    "\n",
    "        output=sample+'_out.txt'\n",
    "        unclassified=sample+'_unclassified.txt'\n",
    "        if is_save_all_outputs: # save unclassified and output files\n",
    "            params={'--db':kk_db,'--threads': threads, '--report': report,'--report-minimizer-data':'','--report-zero-counts':'','--unclassified-out': unclassified, file: '','--output': output }\n",
    "        else :\n",
    "            params={'--db':kk_db,'--threads': threads, '--report': report,'--report-minimizer-data':'','--report-zero-counts':'','--unclassified-out': '/dev/null', file: '','--output': '/dev/null' }\n",
    "        cmd=utils.build_command(tool+'/kraken2',params)\n",
    "        print(cmd)\n",
    "        subprocess.run(cmd.split(),stdout=subprocess.PIPE)\n",
    "\n",
    "        if os.path.exists(report):\n",
    "            hdfs.copy_to_hdfs(report,os.path.join(outputRoot,'report'),overwrite=True)\n",
    "            os.remove(report)\n",
    "            \n",
    "            if is_save_all_outputs:   \n",
    "                # compress\n",
    "                c_output=compress_file(output)            \n",
    "                c_unclassified=compress_file(unclassified)             \n",
    "                # copy to hdfs\n",
    "                hdfs.copy_to_hdfs(c_unclassified,os.path.join(outputRoot,'unclassified'),overwrite=True)\n",
    "                hdfs.copy_to_hdfs(c_output,os.path.join(outputRoot,'output'),overwrite=True)                     \n",
    "                # remove local files\n",
    "                os.remove(output)\n",
    "                os.remove(unclassified)\n",
    "                os.remove(c_output)\n",
    "                os.remove(c_unclassified)\n",
    "\n",
    "        os.remove(file)\n",
    "        return file\n",
    "    else :\n",
    "        print('skipping existing file: ', file)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all input file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "inputFiles=utils.load_file_names(inputRoot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelize\n",
    "rdd=sc.parallelize(inputFiles)\n",
    "# run\n",
    "final=rdd.map( lambda x: apply_kraken_single(x,kk_db_path) ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}