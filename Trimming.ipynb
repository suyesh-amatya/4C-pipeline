{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality trimming of raw FASTQ files\n",
    "This notebook is for quality trimming of raw fastq files. The raw files are expected to be already uploaded in the Hopsworks HDFS. It applies `trimmomatic` and `cutadapt` tools. For trimmomatic, both single and paired end mode is used according to the sample name. (if sample name contains `_R` its treated as paired file else as single ended). For cutadapt processing only **paired R2** files are used as input. Different arguments can be set in `settings.yml` file.\n",
    "\n",
    "Note: Currently only RNA sequence processing is supported via  value of flag `IS_RNA` set to `True`. DNA sequencing is not yet supported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "from hops import hdfs\n",
    "from pyspark import SparkContext\n",
    "\n",
    "import utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=utils.load_arguments(sys.argv)\n",
    "\n",
    "\n",
    "if args is not None:\n",
    "    args=args[utils.KEY_TRIMMOMATIC]\n",
    "else :\n",
    "    sys.exit(utils.NO_CONFIG_ERR)\n",
    "    \n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "\n",
    "OUTPUT_PAIRED=args['OUTPUT_PAIRED']\n",
    "OUTPUT_UNPAIRED=args['OUTPUT_UNPAIRED']\n",
    "IS_SAVE_UNPAIRED= args['SAVE_UNPAIRED']\n",
    "INPUT_ROOT=args['INPUT_ROOT']\n",
    "USE_CUTADAPT = args['USE_CUTADAPT']\n",
    "IS_RNA = args['IS_RNA']\n",
    "LOGS_ROOT = args['LOGS_ROOT']\n",
    "\n",
    "PHRED = args['PHRED']\n",
    "LEADING = str(args['LEADING'])\n",
    "TRAILING = str(args['TRAILING'])\n",
    "SLIDING_WINDOW = str(args['SLIDINGWINDOW'])\n",
    "MIN_LEN = str(args['MINLEN'])\n",
    "THREADS = args['THREADS']\n",
    "JAVA = \"java -jar\"\n",
    "CUTADPAT_ARGS='cutadapt -j 0 -u 3 -o'\n",
    "SPACE=utils.SPACE\n",
    "OUTPUT_SINGLE=args['OUTPUT_SINGLE']\n",
    "\n",
    "#### Get trimmomatic jar and adapter files\n",
    "JAR_PATH=args['JAR']\n",
    "ADAPTER_PAIR_PATH=args['ADAPTER_PAIR']\n",
    "ADAPTER_SINGLE_PATH=args['ADAPTER_SINGLE']\n",
    "if JAR_PATH is None:\n",
    "    sys.exit(utils.TRIMMOMATIC_NOT_FOUND)\n",
    "\n",
    "if (ADAPTER_PAIR_PATH or ADAPTER_SINGLE_PATH) is None:\n",
    "    sys.exit(utils.TRIMMOMATIC_ADAPTER_NOT_FOUND)\n",
    "\n",
    "tool=os.path.basename(JAR_PATH)\n",
    "ADAPTER_PAIR=os.path.basename(ADAPTER_PAIR_PATH)\n",
    "ADAPTER_SINGLE=os.path.basename(ADAPTER_SINGLE_PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "cutadpat\n",
    "\"\"\"\n",
    "def cut(input_file,log_file):\n",
    "    \n",
    "    out_trim='cut_'+input_file\n",
    "    params={out_trim : input_file}\n",
    "    cmd_cut=utils.build_command(CUTADPAT_ARGS, params)\n",
    "    with open(log_file, \"a\") as f:\n",
    "        subprocess.run(cmd_cut.split(' '),stdout=f,stderr=f)\n",
    "    if out_trim:\n",
    "        os.remove(input_file)\n",
    "        os.rename(out_trim,input_file)\n",
    "        \n",
    "    params.clear()\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Trimmomatic on single end file.\n",
    "Output files are copied back to hdfs\n",
    "\"\"\"\n",
    "def apply_trim_single(file_input):\n",
    "\n",
    "    # get file name\n",
    "    file_name=os.path.split(file_input)[1]\n",
    "    file_output='trim_'+file_name\n",
    "\n",
    "    # check if output already exists\n",
    "    if hdfs.exists(os.path.join(OUTPUT_SINGLE,file_name)):\n",
    "        print('Skipping input file as output file already exists', file_name)\n",
    "        return [-1]\n",
    "\n",
    "    # copy input to local\n",
    "    hdfs.copy_to_local(file_input, overwrite=True)\n",
    "    if not (os.path.exists(tool)):\n",
    "        hdfs.copy_to_local(JAR_PATH)\n",
    "    if not (os.path.exists(ADAPTER_SINGLE)):\n",
    "        hdfs.copy_to_local(ADAPTER_SINGLE_PATH)\n",
    "\n",
    "    # single end attributes\n",
    "    attribute='SE -'+PHRED\n",
    "    threads='-threads '+str(THREADS)\n",
    "    illuminaclip_adapters = \"ILLUMINACLIP:\"+ADAPTER_SINGLE+\":2:30:10\"\n",
    "    illuminaclip_Attribute = \"LEADING:\"+LEADING+SPACE+\"TRAILING:\"+TRAILING+SPACE+\"SLIDINGWINDOW:\"+SLIDING_WINDOW+SPACE+\"MINLEN:\"+MIN_LEN\n",
    "    s=SPACE\n",
    "    # command to run\n",
    "    cmd_single =JAVA + s + tool + s + attribute + s + threads + s + file_name + s + file_output + s + illuminaclip_adapters + s + illuminaclip_Attribute\n",
    "    print('INFO: Run trimmomatic command: ', cmd_single)\n",
    "    # run\n",
    "    log_file=utils.get_sampleName_with_lane(file_name)+'.txt'\n",
    "    # run\n",
    "    with open(log_file, \"w\") as f:\n",
    "        result=subprocess.run(cmd_single.split(),stdout=f,stderr=f)\n",
    "  \n",
    "    status=False\n",
    "    if os.path.exists(file_output) and result.returncode==0:\n",
    "        # copy output to hdfs\n",
    "        hdfs.copy_to_hdfs(file_output, OUTPUT_SINGLE, overwrite=True)  \n",
    "\n",
    "        status=True\n",
    "\n",
    "    # copy logs\n",
    "    hdfs.copy_to_hdfs(log_file, LOGS_ROOT, overwrite=True)\n",
    "    # remove local files\n",
    "    os.remove(file_output)\n",
    "    os.remove(file_name)    \n",
    "    return file_output\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Trimmomatic on paired end file via subprocess.\n",
    "Output files are copied to hdfs.\n",
    "\"\"\"\n",
    "def apply_trim_paired(x):\n",
    "    r1=x[0] # R1\n",
    "    r2=x[1] # R2\n",
    "\n",
    "    # get file names\n",
    "    filename_forward=os.path.basename(r1)\n",
    "    filename_reverse=os.path.basename(r2)\n",
    "    # append suffixes to output\n",
    "    output_forward_paired =utils.TRIM_PAIRED+filename_forward\n",
    "    output_forward_unpaired=utils.TRIM_UNPAIRED+filename_forward\n",
    "    output_reverse_paired =utils.TRIM_PAIRED+filename_reverse\n",
    "    output_reverse_unpaired=utils.TRIM_UNPAIRED+filename_reverse\n",
    "\n",
    "    # check if output already exists\n",
    "    if hdfs.exists(os.path.join(OUTPUT_PAIRED,output_forward_paired)):\n",
    "        print('Skipping input file as output file already exists', filename_forward)\n",
    "        return [-1]\n",
    "\n",
    "    hdfs.copy_to_local(r1, overwrite=True)\n",
    "    hdfs.copy_to_local(r2, overwrite=True)\n",
    "    if not (os.path.exists(tool)):\n",
    "        hdfs.copy_to_local(JAR_PATH)\n",
    "    if not (os.path.exists(ADAPTER_PAIR)):\n",
    "        hdfs.copy_to_local(ADAPTER_PAIR_PATH)\n",
    "\n",
    "\n",
    "    ### paired end attributes\n",
    "    attribute='PE -'+PHRED\n",
    "    threads='-threads '+str(THREADS)\n",
    "    illuminaclip_adapters = \"ILLUMINACLIP:\"+ADAPTER_PAIR+\":2:30:10:2:keepBothReads\"\n",
    "    illuminaclip_Attribute = \"LEADING:\"+LEADING+SPACE+\"TRAILING:\"+TRAILING+SPACE+\"SLIDINGWINDOW:\"+SLIDING_WINDOW+SPACE+\"MINLEN:\"+MIN_LEN\n",
    "\n",
    "    s=SPACE\n",
    "    cmd1 = JAVA + s + tool + s + attribute + s + threads + s + filename_forward + s + filename_reverse + s + output_forward_paired + s + output_forward_unpaired\n",
    "    cmd2 = s + output_reverse_paired + s + output_reverse_unpaired + s + illuminaclip_adapters + s + illuminaclip_Attribute\n",
    "    # final command\n",
    "    cmd_paired = cmd1 + cmd2\n",
    "    print('INFO: Run trimmomatic command: ', cmd_paired)\n",
    "    # run\n",
    "    log_file=utils.get_sampleName_with_lane(filename_forward)+'.txt'\n",
    "    with open(log_file, \"w\") as f:\n",
    "        result=subprocess.run(cmd_paired.split(),stdout=f,stderr=f)\n",
    "\n",
    "  \n",
    "    status=False\n",
    "    if result.returncode==0:\n",
    "        if USE_CUTADAPT: # run cutadapt on R2 \n",
    "            cut(output_reverse_paired,log_file)        \n",
    "            \n",
    "        # copy output to hdfs\n",
    "        hdfs.copy_to_hdfs(output_forward_paired, OUTPUT_PAIRED, overwrite=True)\n",
    "        hdfs.copy_to_hdfs(output_reverse_paired, OUTPUT_PAIRED, overwrite=True)\n",
    "        if IS_SAVE_UNPAIRED:\n",
    "            hdfs.copy_to_hdfs(output_forward_unpaired, OUTPUT_UNPAIRED, overwrite=True)\n",
    "            hdfs.copy_to_hdfs(output_reverse_unpaired, OUTPUT_UNPAIRED, overwrite=True)\n",
    "\n",
    "        status=True\n",
    "\n",
    "    # copy logs\n",
    "    hdfs.copy_to_hdfs(log_file, LOGS_ROOT, overwrite=True)\n",
    "    # remove local files\n",
    "    os.remove(output_reverse_paired) \n",
    "    os.remove(output_forward_unpaired)\n",
    "    os.remove(output_forward_paired) \n",
    "    os.remove(output_reverse_unpaired) \n",
    "    os.remove(filename_forward)\n",
    "    os.remove(filename_reverse)\n",
    "                \n",
    " \n",
    "    return [status,output_forward_paired,output_forward_unpaired,output_reverse_paired,output_reverse_unpaired]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load input files hdfs path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files=utils.load_file_names(INPUT_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list of all single end files and run trimmomatic in single mode in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### single\n",
    "single_files=[f for f in all_files if utils.R_IDENTIFIER not in f]\n",
    "\n",
    "print('number of single input  files processing ', len(single_files))\n",
    "dataRdd=sc.parallelize(single_files)\n",
    "\n",
    "# run\n",
    "trimmedSingleFiles=dataRdd.map(lambda x: apply_trim_single(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pair R1 and R2 as a tuple in a list and run trimmomatic in paired end in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pair R1 and R2\n",
    "pairedList =  utils.group_R1R2(all_files)\n",
    "print('number of input paired files processing ', len(pairedList))\n",
    "dataPairedRdd=sc.parallelize(pairedList)\n",
    "# run\n",
    "trimmedFiles=dataPairedRdd.map(lambda x: apply_trim_paired(x)).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-fd840124",
   "language": "python",
   "display_name": "PyCharm (ki-pipeline)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}