{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter non human\n",
    "\n",
    "This job to filter non human unmapped reads from BAM file via samtools. It needs a BED file to filter specific reads regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import os\n",
    "from hops import hdfs\n",
    "import utils\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args=utils.load_arguments([0,'hdfs:///Projects/TCGA_viruses/Jupyter/pipeline/settings_DJ.yml'])\n",
    "args=utils.load_arguments(sys.argv)\n",
    "if args is not None:\n",
    "    args=args['Unhuman']\n",
    "else :\n",
    "    sys.exit(utils.NO_CONFIG_ERR)\n",
    "    \n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "\n",
    "inputRoot=args['INPUT_ROOT']\n",
    "outputBam=args['OUTPUT_BAM']\n",
    "humanFilterPath=args['FILTER_BED']\n",
    "\n",
    "threads=str(args['THREADS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### map function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Runs pysam on a BAM file to filter specific reads region as specified in BED file.\n",
    "Output is copied back to hdfs.\n",
    "If output file name is already present in destination the process is skipped \n",
    "to avoid processing of same file incase of resubmit of failed run.\n",
    "\"\"\"\n",
    "def remove_human(file):\n",
    "    \n",
    "    filename=os.path.basename(file)\n",
    "    bam_file=filename.split('.')[0]+'_NH.bam'\n",
    "    if not hdfs.exists(os.path.join(outputBam,bam_file)): # check if output already exists\n",
    "    \n",
    "        hdfs.copy_to_local(humanFilterPath)\n",
    "        humanFilter=os.path.basename(humanFilterPath)\n",
    "        hdfs.copy_to_local(file, overwrite=True)        \n",
    "        print(\"INFO: Run non human BAM : \", filename)\n",
    "        pysam.view('-o','/dev/null', '-L', humanFilter, '-U',bam_file, filename,'-@',threads, catch_stdout=False)\n",
    "\n",
    "        if os.path.exists(bam_file):\n",
    "            hdfs.copy_to_hdfs(bam_file,outputBam,overwrite=True)\n",
    "            os.remove(bam_file)\n",
    "            os.remove(filename)\n",
    "\n",
    "        return bam_file\n",
    "    \n",
    "    else:\n",
    "        print('skipping existing file: ', filename)            \n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load input files hdfs paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputFiles=utils.load_file_names(inputRoot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of input files: ', len(inputFiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### make spark rdd and map function\n",
    "unMapped=sc.parallelize(inputFiles).map(remove_human).collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}